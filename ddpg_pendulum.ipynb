{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeBh84QkYH02Xfme/MQBbG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gopalapare/ddpg/blob/main/ddpg_pendulum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "KUukgW0ARZgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem = \"Pendulum-v1\"\n",
        "env = gym.make(problem)\n",
        "\n",
        "num_states = env.observation_space.shape[0]\n",
        "print(\"Size of State Space ->  {}\".format(num_states))\n",
        "num_actions = env.action_space.shape[0]\n",
        "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
        "\n",
        "upper_bound = env.action_space.high[0]\n",
        "lower_bound = env.action_space.low[0]\n",
        "\n",
        "\n",
        "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
        "print(\"Min Value of Action ->  {}\".format(lower_bound))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsnE0k6TcjvF",
        "outputId": "4f5443fe-4a87-47f5-da08-088f5d03836a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of State Space ->  3\n",
            "Size of Action Space ->  1\n",
            "Max Value of Action ->  2.0\n",
            "Min Value of Action ->  -2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        )\n",
        "        # Store x into x_prev\n",
        "        # Makes next noise dependent on current one\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)"
      ],
      "metadata": {
        "id": "3CkS6Xh5dkBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tf.function\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
        "    ):\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = target_actor(next_state_batch, training=True)\n",
        "            y = reward_batch + gamma * target_critic(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            )\n",
        "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch, training=True)\n",
        "            critic_value = critic_model([state_batch, actions], training=True)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
        "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "\n",
        "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tf.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))"
      ],
      "metadata": {
        "id": "NJuQh-9CGQe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_actor():\n",
        "    # Initialize weights between -3e-3 and 3-e3\n",
        "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
        "\n",
        "    # Our upper bound is 2.0 for Pendulum.\n",
        "    outputs = outputs * upper_bound\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
        "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
        "\n",
        "    # Both are passed through seperate layer before concatenating\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1)(out)\n",
        "\n",
        "    # Outputs single value for give state-action\n",
        "    model = tf.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Fev7Dr-udlEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy(state, noise_object):\n",
        "    sampled_actions = tf.squeeze(actor_model(state))\n",
        "    noise = noise_object()\n",
        "    # Adding noise to action\n",
        "    sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "    # We make sure action is within bounds\n",
        "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
        "\n",
        "    return [np.squeeze(legal_action)]"
      ],
      "metadata": {
        "id": "QcTBEVlNGpbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std_dev = 0.2\n",
        "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
        "\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.002\n",
        "actor_lr = 0.001\n",
        "\n",
        "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "total_episodes = 100\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.005\n",
        "\n",
        "buffer = Buffer(50000, 64)"
      ],
      "metadata": {
        "id": "KR8f2OHYGq2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "# Takes about 4 min to train\n",
        "for ep in range(total_episodes):\n",
        "\n",
        "    prev_state = env.reset()\n",
        "    episodic_reward = 0\n",
        "\n",
        "    while True:\n",
        "        # Uncomment this to see the Actor in action\n",
        "        # But not in a python notebook.\n",
        "        # env.render()\n",
        "\n",
        "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
        "\n",
        "        action = policy(tf_prev_state, ou_noise)\n",
        "        # Recieve state and reward from environment.\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        buffer.record((prev_state, action, reward, state))\n",
        "        episodic_reward += reward\n",
        "\n",
        "        buffer.learn()\n",
        "        update_target(target_actor.variables, actor_model.variables, tau)\n",
        "        update_target(target_critic.variables, critic_model.variables, tau)\n",
        "\n",
        "        # End this episode when `done` is True\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        prev_state = state\n",
        "\n",
        "    ep_reward_list.append(episodic_reward)\n",
        "\n",
        "    # Mean of last 40 episodes\n",
        "    avg_reward = np.mean(ep_reward_list[-40:])\n",
        "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
        "    avg_reward_list.append(avg_reward)\n",
        "\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "plt.plot(avg_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4fWVHna3GxwM",
        "outputId": "1fbada22-6dba-4729-903f-1c7cfa82b851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode * 0 * Avg Reward is ==> -1512.7446973987633\n",
            "Episode * 1 * Avg Reward is ==> -1301.9140330365312\n",
            "Episode * 2 * Avg Reward is ==> -1375.5730713044795\n",
            "Episode * 3 * Avg Reward is ==> -1390.691179577014\n",
            "Episode * 4 * Avg Reward is ==> -1346.1883599846947\n",
            "Episode * 5 * Avg Reward is ==> -1396.8632497989665\n",
            "Episode * 6 * Avg Reward is ==> -1416.658438658727\n",
            "Episode * 7 * Avg Reward is ==> -1416.0688454942983\n",
            "Episode * 8 * Avg Reward is ==> -1392.1775994717811\n",
            "Episode * 9 * Avg Reward is ==> -1350.3580045782371\n",
            "Episode * 10 * Avg Reward is ==> -1314.8514996444128\n",
            "Episode * 11 * Avg Reward is ==> -1280.8991568625713\n",
            "Episode * 12 * Avg Reward is ==> -1256.4552444086144\n",
            "Episode * 13 * Avg Reward is ==> -1185.1863139734762\n",
            "Episode * 14 * Avg Reward is ==> -1132.1626593826381\n",
            "Episode * 15 * Avg Reward is ==> -1077.2348701227509\n",
            "Episode * 16 * Avg Reward is ==> -1021.6375064452809\n",
            "Episode * 17 * Avg Reward is ==> -986.8645689594297\n",
            "Episode * 18 * Avg Reward is ==> -941.5383708306755\n",
            "Episode * 19 * Avg Reward is ==> -906.4747776255732\n",
            "Episode * 20 * Avg Reward is ==> -875.0773933271488\n",
            "Episode * 21 * Avg Reward is ==> -846.0128572785986\n",
            "Episode * 22 * Avg Reward is ==> -820.235330197678\n",
            "Episode * 23 * Avg Reward is ==> -799.9856402116156\n",
            "Episode * 24 * Avg Reward is ==> -773.0588841959735\n",
            "Episode * 25 * Avg Reward is ==> -748.1289677249881\n",
            "Episode * 26 * Avg Reward is ==> -724.7493485001827\n",
            "Episode * 27 * Avg Reward is ==> -707.5824214074465\n",
            "Episode * 28 * Avg Reward is ==> -687.668843111175\n",
            "Episode * 29 * Avg Reward is ==> -679.9350430692938\n",
            "Episode * 30 * Avg Reward is ==> -661.8767568454447\n",
            "Episode * 31 * Avg Reward is ==> -650.5259345607531\n",
            "Episode * 32 * Avg Reward is ==> -634.4420168265522\n",
            "Episode * 33 * Avg Reward is ==> -623.067563013098\n",
            "Episode * 34 * Avg Reward is ==> -608.5657330223032\n",
            "Episode * 35 * Avg Reward is ==> -591.817369181585\n",
            "Episode * 36 * Avg Reward is ==> -579.3481344808599\n",
            "Episode * 37 * Avg Reward is ==> -567.2908376880494\n",
            "Episode * 38 * Avg Reward is ==> -555.9890338398931\n",
            "Episode * 39 * Avg Reward is ==> -545.2007031878242\n",
            "Episode * 40 * Avg Reward is ==> -510.6250797792627\n",
            "Episode * 41 * Avg Reward is ==> -489.71495721776165\n",
            "Episode * 42 * Avg Reward is ==> -454.8235072934855\n",
            "Episode * 43 * Avg Reward is ==> -421.81449955796745\n",
            "Episode * 44 * Avg Reward is ==> -399.08316290508316\n",
            "Episode * 45 * Avg Reward is ==> -365.12402102558184\n",
            "Episode * 46 * Avg Reward is ==> -333.3038673856892\n",
            "Episode * 47 * Avg Reward is ==> -301.4941996418812\n",
            "Episode * 48 * Avg Reward is ==> -283.9682616694721\n",
            "Episode * 49 * Avg Reward is ==> -269.1660502176071\n",
            "Episode * 50 * Avg Reward is ==> -254.31534320722986\n",
            "Episode * 51 * Avg Reward is ==> -238.12373360032058\n",
            "Episode * 52 * Avg Reward is ==> -217.46926475788686\n",
            "Episode * 53 * Avg Reward is ==> -214.4273174602964\n",
            "Episode * 54 * Avg Reward is ==> -207.93427637933118\n",
            "Episode * 55 * Avg Reward is ==> -207.78348871097805\n",
            "Episode * 56 * Avg Reward is ==> -213.1727228933797\n",
            "Episode * 57 * Avg Reward is ==> -206.54711365160105\n",
            "Episode * 58 * Avg Reward is ==> -206.7408731098588\n",
            "Episode * 59 * Avg Reward is ==> -203.80883697653826\n",
            "Episode * 60 * Avg Reward is ==> -200.59266359980978\n",
            "Episode * 61 * Avg Reward is ==> -197.8413041057383\n",
            "Episode * 62 * Avg Reward is ==> -197.67507484028562\n",
            "Episode * 63 * Avg Reward is ==> -195.36619642270358\n",
            "Episode * 64 * Avg Reward is ==> -199.67619691942883\n",
            "Episode * 65 * Avg Reward is ==> -199.71626721559053\n",
            "Episode * 66 * Avg Reward is ==> -197.03248443543862\n",
            "Episode * 67 * Avg Reward is ==> -194.22333498530847\n",
            "Episode * 68 * Avg Reward is ==> -196.62438671546357\n",
            "Episode * 69 * Avg Reward is ==> -188.61460619740214\n",
            "Episode * 70 * Avg Reward is ==> -193.56544557319899\n",
            "Episode * 71 * Avg Reward is ==> -189.18974446156258\n",
            "Episode * 72 * Avg Reward is ==> -192.3271420317983\n",
            "Episode * 73 * Avg Reward is ==> -189.5296772635454\n",
            "Episode * 74 * Avg Reward is ==> -186.93833465391884\n",
            "Episode * 75 * Avg Reward is ==> -189.9895896567828\n",
            "Episode * 76 * Avg Reward is ==> -186.89727096094393\n",
            "Episode * 77 * Avg Reward is ==> -184.0953990076586\n",
            "Episode * 78 * Avg Reward is ==> -184.22459440170073\n",
            "Episode * 79 * Avg Reward is ==> -184.2394615672722\n",
            "Episode * 80 * Avg Reward is ==> -184.25525863294652\n",
            "Episode * 81 * Avg Reward is ==> -180.97966048825876\n",
            "Episode * 82 * Avg Reward is ==> -177.99963364264426\n",
            "Episode * 83 * Avg Reward is ==> -178.3950880614702\n",
            "Episode * 84 * Avg Reward is ==> -171.99544483736403\n",
            "Episode * 85 * Avg Reward is ==> -172.58417530559714\n",
            "Episode * 86 * Avg Reward is ==> -175.40013792587598\n",
            "Episode * 87 * Avg Reward is ==> -175.1443579380879\n",
            "Episode * 88 * Avg Reward is ==> -169.1666426092649\n",
            "Episode * 89 * Avg Reward is ==> -169.01525451791477\n",
            "Episode * 90 * Avg Reward is ==> -163.01943017829973\n",
            "Episode * 91 * Avg Reward is ==> -156.7180015144474\n",
            "Episode * 92 * Avg Reward is ==> -159.426622030629\n",
            "Episode * 93 * Avg Reward is ==> -159.08212534254395\n",
            "Episode * 94 * Avg Reward is ==> -159.18221768592383\n",
            "Episode * 95 * Avg Reward is ==> -158.87104338627222\n",
            "Episode * 96 * Avg Reward is ==> -156.76349228062185\n",
            "Episode * 97 * Avg Reward is ==> -159.12156229115527\n",
            "Episode * 98 * Avg Reward is ==> -155.83323223969256\n",
            "Episode * 99 * Avg Reward is ==> -156.04824890756103\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the weights\n",
        "actor_model.save_weights(\"pendulum_actor.h5\")\n",
        "critic_model.save_weights(\"pendulum_critic.h5\")\n",
        "\n",
        "target_actor.save_weights(\"pendulum_target_actor.h5\")\n",
        "target_critic.save_weights(\"pendulum_target_critic.h5\")"
      ],
      "metadata": {
        "id": "FJeTpHjoIch9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a=np.array([2,3,4,6])\n",
        "b=a.shape[0]\n",
        "m=np.zeros_like([3,5,7])\n",
        "m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUB5QuBx9cvS",
        "outputId": "06bb5882-9e06-4ae0-8c75-32309408fdb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import mpctools as mpc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "sys.path.insert(0, '/home/rui/Documents/RL_vs_MPC/Models')\n",
        "sys.path.insert(0, '/home/rui/Documents/RL_vs_MPC/RL_Codes')\n",
        "sys.path.insert(0, '/Users/ruinian/Documents/Research/Modules')\n",
        "sys.path.insert(0, '/home/rui/Documents/Research/Modules')\n",
        "\n",
        "from RL_Module_Updated import *\n",
        "\n",
        "\"\"\"\n",
        "Define reward function for RL.  User defines the reward function structure.  The below is an example.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Model for MechaTronix\n",
        "def model(action):\n",
        "    \"\"\"\n",
        "    Action: Total pump frequency (not delta pump frequency)\n",
        "    \"\"\"\n",
        "    return 0.01158682 * np.square(action) + 0.02409036 * action - 2.073161\n",
        "\n",
        "\n",
        "# Reward function\n",
        "def reward_calc(state, setpoint):\n",
        "    return max(-np.square(state - setpoint), -150)\n",
        "\n",
        "\n",
        "def simulation():\n",
        "\n",
        "    # Reinforcement Learning Initiation\n",
        "    rl = ReinforceLearning(discount_factor=0.95, states_start=300, states_stop=340, states_interval=0.5,\n",
        "                           actions_start=-15, actions_stop=15, actions_interval=2.5, learning_rate=0.5,\n",
        "                           epsilon=0.2, doe=1.2, eval_period=5)\n",
        "\n",
        "    \"\"\"\n",
        "    Example of user defined states and actions.  Users do not need to do this.  This is only if users want to define \n",
        "    their own states and actions.  RL will automatically populate states and actions if user does not input their own.\n",
        "    \"\"\"\n",
        "\n",
        "    states = []\n",
        "\n",
        "    rl.x1 = np.linspace(-20, 20, 21)\n",
        "    rl.x2 = np.linspace(1, 3, 3)\n",
        "\n",
        "    for i in rl.x1:\n",
        "        for j in rl.x2:\n",
        "            states.append([i, j])\n",
        "\n",
        "    rl.user_states(list(states))\n",
        "\n",
        "    actions = np.linspace(-10, 10, 21)\n",
        "    rl.user_actions(list(actions))\n",
        "\n",
        "    \"\"\"\n",
        "    Load pre-trained Q, T and NT matrices\n",
        "    \"\"\"\n",
        "\n",
        "    q = np.loadtxt(\"Q_Matrix.txt\")\n",
        "    t = np.loadtxt(\"T_Matrix.txt\")\n",
        "    nt = np.loadtxt(\"NT_Matrix.txt\")\n",
        "\n",
        "    rl.user_matrices(q, t, nt)\n",
        "\n",
        "    \"\"\"\n",
        "    Simulation portion\n",
        "    \"\"\"\n",
        "\n",
        "    episodes = 1\n",
        "    num_sim = 100\n",
        "    rlist = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "\n",
        "        # Environment parameters\n",
        "        env_states = np.zeros((num_sim + 1, 2))\n",
        "        env_states[0, :] = [10.5, 2]\n",
        "\n",
        "        error = np.zeros(num_sim + 1)\n",
        "\n",
        "        env_actions = np.zeros(num_sim + 1)\n",
        "        env_actions[0] = 32\n",
        "\n",
        "        # Reset the model after each episode\n",
        "        tot_reward = 0\n",
        "\n",
        "        # State and action indices\n",
        "        state = 0\n",
        "        action = 0\n",
        "\n",
        "        if episode % 22 == 0:\n",
        "            cur_setpoint = 25     # np.random.uniform(0, 60)\n",
        "        else:\n",
        "            cur_setpoint = 25     # np.random.uniform(0, 60)\n",
        "\n",
        "        for t in range(1, num_sim + 1):\n",
        "\n",
        "            \"\"\"\n",
        "            Set-point\n",
        "            \"\"\"\n",
        "\n",
        "            error[t] = env_states[t - 1, 0] - cur_setpoint\n",
        "\n",
        "            \"\"\"\n",
        "            Disturbance\n",
        "            \"\"\"\n",
        "\n",
        "            # if t == 10:\n",
        "            #     model.disturbance()\n",
        "\n",
        "            \"\"\"\n",
        "            RL Evaluate\n",
        "            \"\"\"\n",
        "\n",
        "            if t % rl.eval_period == 0:\n",
        "                # State: index of state; actions: Physical action; action: index of action\n",
        "                state, env_actions[t], action = rl.action_selection(np.array([error[t], env_states[t - 1, 1]]),\n",
        "                                                                    env_actions[t - 1],\n",
        "                                                                    25, ep_greedy=False, time=t,\n",
        "                                                                    min_eps_rate=0.9)\n",
        "\n",
        "            else:\n",
        "                env_actions[t] = env_actions[t - 1]\n",
        "\n",
        "            # State evolution/trajectory\n",
        "            env_states[t, 0] = model(env_actions[t])  # + np.random.uniform(-0.1, 0.1)\n",
        "\n",
        "            if env_states[t, 0] < 10:\n",
        "                env_states[t, 1] = 1\n",
        "            elif 10 <= env_states[t, 0] < 20:\n",
        "                env_states[t, 1] = 2\n",
        "            else:\n",
        "                env_states[t, 1] = 3\n",
        "\n",
        "            \"\"\"\n",
        "            Feedback evaluation\n",
        "            \"\"\"\n",
        "\n",
        "            if t == rl.eval_feedback:\n",
        "                reward = reward_calc(env_states[t, 0], cur_setpoint)\n",
        "                rl.matrix_update(action, reward, state, np.array([error[t], env_states[t, 1]]), 5)\n",
        "                tot_reward = tot_reward + reward\n",
        "\n",
        "        rlist.append(tot_reward)\n",
        "\n",
        "        rl.autosave(episode, 5)\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print('The current error is: {:2f}'.format(np.sqrt(np.sum(np.square(env_states[:, 0] - cur_setpoint)))))\n",
        "\n",
        "    # Plotting\n",
        "    fonts = {\"family\": \"serif\",\n",
        "             \"weight\": \"normal\",\n",
        "             \"size\": \"12\"}\n",
        "\n",
        "    plt.rc('font', **fonts)\n",
        "    plt.rc('text', usetex=True)\n",
        "\n",
        "    plt.plot(env_states[:, 0])\n",
        "    plt.xlabel('Pump RPM, (Hertz)')\n",
        "    plt.ylabel('Pressure, P')\n",
        "\n",
        "    plt.axhline(y=cur_setpoint, color='red')\n",
        "\n",
        "    plt.text(x=750, y=34.5, s='Set point')\n",
        "    plt.text(x=750, y=35.9, s='Pressure Trajectory')\n",
        "\n",
        "    # plt.ylim([33, 43])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    #return model, rl, rlist, env_states, env_actions\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "simulation()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "AJLuZk-HdlOV",
        "outputId": "35470232-516c-4b2a-f484-1147ed731f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9d5e99ea30e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;31m# if __name__ == \"__main__\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m \u001b[0msimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-9d5e99ea30e9>\u001b[0m in \u001b[0;36msimulation\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m                                                                     \u001b[0menv_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                                                                     \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_greedy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                                                                     min_eps_rate=0.9)\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/RL_Module_Updated.py\u001b[0m in \u001b[0;36maction_selection\u001b[0;34m(self, cur_state, last_input, no_decay, ep_greedy, time, min_eps_rate)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \"\"\"\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mq_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/RL_Module_Updated.py\u001b[0m in \u001b[0;36mstate_detection\u001b[0;34m(self, cur_state)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstate_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cur_state'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx_current\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_current\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.layers.InputLayer(\n",
        "    input_shape=None,\n",
        "    batch_size=None,\n",
        "    dtype=None,\n",
        "    input_tensor=None,\n",
        "    sparse=None,\n",
        "    name=None,\n",
        "    ragged=None,\n",
        "    type_spec=None,\n",
        ")\n",
        "# With explicit InputLayer.\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(4,)),\n",
        "  tf.keras.layers.Dense(8)])\n",
        "model.compile(tf.keras.optimizers.RMSprop(0.001), loss='mse')\n",
        "model.fit(np.zeros((10, 4)),\n",
        "          np.ones((10, 8)))\n",
        "\n",
        "# Without InputLayer and let the first layer to have the input_shape.\n",
        "# Keras will add a input for the model behind the scene.\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(8, input_shape=(4,))])\n",
        "model.compile(tf.keras.optimizers.RMSprop(0.001), loss='mse')\n",
        "model.fit(np.zeros((10, 4)),\n",
        "          np.ones((10, 8)))"
      ],
      "metadata": {
        "id": "uECOdTjDdljN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9e59102-adc6-48b6-d555-cbcacbafc469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 571ms/step - loss: 1.0000\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f119a65eb10>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}